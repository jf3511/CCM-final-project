{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ## **SET UP PYGAME ENVIRONMENT**\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# ONLY NEED TO RUN ONCE\n",
    "#import os\n",
    "#get_ipython().system('git clone https://github.com/ntasfi/PyGame-Learning-Environment/')\n",
    "#os.chdir(\"PyGame-Learning-Environment\")\n",
    "#print(f\"Current directory {os.getcwd()}\")\n",
    "#get_ipython().system('pip install -e .')\n",
    "#get_ipython().system('pip install pygame')\n",
    "#get_ipython().system('pip install -q tensorflow')\n",
    "#get_ipython().system('pip install -q keras')\n",
    "\n",
    "\n",
    "# # Imports\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import platform\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from collections import deque\n",
    "#import h5py\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from keras import Sequential\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers\n",
    "from ple import PLE\n",
    "from ple.games.reverse_updown_pixelcopter_RL import Pixelcopter\n",
    "\n",
    "\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version} {platform.system()}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU')) > 0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")\n",
    "\n",
    "# Sets the initial window position of the game\n",
    "x = 1200  \n",
    "y = 200\n",
    "os.environ['SDL_VIDEO_WINDOW_POS'] = \"%d,%d\" % (x, y)\n",
    "\n",
    "\n",
    "# Own Tensorboard class\n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        self.log_write_dir = self.log_dir\n",
    "        self._train_dir = os.path.join(self.log_dir + 'train')\n",
    "        self._val_dir = os.path.join(self.log_dir, 'validation')\n",
    "        self._should_write_train_graph = False\n",
    "\n",
    "    # Overriding this method to stop creating default log writer\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        self._train_step = self.model._train_counter\n",
    "        self._val_step = self.model._test_counter\n",
    "\n",
    "    # Overrides, saves logs with our step number\n",
    "    # (otherwise every .fit() will start writing from 0th step)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    # Overrides\n",
    "    # We train for one batch only, no need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Overrides, so won't close writer\n",
    "    def on_train_end(self, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Custom method for saving own metrics\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    def update_stats(self, **stats):\n",
    "        with self.writer.as_default():\n",
    "            for key, value in stats.items():\n",
    "                tf.summary.scalar(key, value, step=self.step)\n",
    "                self.writer.flush()\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    # hyper-parameters\n",
    "    INPUT_SIZE = 7\n",
    "    OUTPUT_SIZE = 2\n",
    "    EPSILON = 1\n",
    "    MIN_EPSILON = 0.05\n",
    "    GAMMA = 0.99\n",
    "    MIN_MEMORY_SIZE = 1000\n",
    "    UPDATE_TARGET_LIMIT = 3\n",
    "\n",
    "    # based on documentation, state has 7 features\n",
    "    # output is 2 dimensions, 0 = do nothing, 1 = jump\n",
    "\n",
    "    def __init__(self, mode=\"train\", nodes=32, memory_size=5000, final_act=\"linear\", minibatch=32,\n",
    "                 lr=1e-2, num_episodes=1000):\n",
    "        # depending on what mode the agent is in, will determine how the agent chooses actions\n",
    "        # if agent is training, EPSILON = 1 and will decay over time with epsilon probability of exploring\n",
    "        # if agent is playing (using trained model), EPSILON = 0 and only choose actions based on Q network\n",
    "        self.DECAY_RATE = 5 / num_episodes\n",
    "        self.HIDDEN_NODES = nodes\n",
    "        self.MEMORY_SIZE = memory_size\n",
    "        self.FINAL_ACTIVATION = final_act\n",
    "        self.MINIBATCH_SIZE = minibatch\n",
    "        self.LEARNING_RATE = lr\n",
    "        self.EPSILON = 1 if mode == \"train\" else 0\n",
    "        self.MODEL_NAME = f\"model - ({lr} {minibatch} {memory_size} {nodes} {final_act} {num_episodes})\"\n",
    "        self.MODEL_FILE = \"dqn/best/model - (0.01 64 10000 49 linear 10000___111.11max___23.35avg___-3.82min.h5\"\n",
    "        # main model  # gets trained every step\n",
    "        self.model = self.create_model(False)\n",
    "        print(self.model.summary())\n",
    "        print(\"Finished building baseline model..\")\n",
    "        self.action_map = {\n",
    "            0: None,\n",
    "            1: 119\n",
    "        }\n",
    "        # Target model this is what we predict against every step\n",
    "        self.target_model = self.create_model(False)\n",
    "        print(\"Finished building target model..\")\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.replay_memory = deque(maxlen=self.MEMORY_SIZE)\n",
    "        self.tensorboard = ModifiedTensorBoard(\n",
    "            log_dir=f\"logs/{self.MODEL_NAME}-{int(time.time())}\") if mode == \"train\" else None\n",
    "        self.target_update_counter = 0\n",
    "        self.rewards = []\n",
    "\n",
    "    def create_model(self, model_file):\n",
    "        if model_file:\n",
    "            print(\"Loading model...\")\n",
    "            model = keras.models.load_model(model_file)\n",
    "            self.EPSILON = 0\n",
    "        else:\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(\n",
    "                14,\n",
    "                input_shape=(self.INPUT_SIZE,),\n",
    "                activation=\"relu\"\n",
    "            ))\n",
    "\n",
    "            model.add(Dense(self.HIDDEN_NODES, activation=\"relu\"))\n",
    "            # model.add(Dropout(0.1))\n",
    "\n",
    "            model.add(Dense(self.OUTPUT_SIZE, activation=self.FINAL_ACTIVATION))  # ACTION_SPACE_SIZE = how many choices (9)\n",
    "            model.compile(\n",
    "                loss=\"mse\",\n",
    "                optimizer=Adam(lr=self.LEARNING_RATE),\n",
    "                metrics=['mae']\n",
    "            )\n",
    "        return model\n",
    "\n",
    "    def update_replay_memory(self, state, action, reward, new_state, done):\n",
    "        self.replay_memory.append((state, action, reward, new_state, done))\n",
    "        if len(self.replay_memory) > self.MEMORY_SIZE:\n",
    "            self.replay_memory.popleft()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # chose random action with probability epsilon\n",
    "        if np.random.uniform() < self.EPSILON:\n",
    "            # to speed up training give higher probability to action 0 (no jump)\n",
    "            action_index = np.random.choice([0, 1], size=1, p=[0.8, 0.2])[0]\n",
    "            # action_index = np.random.randint(self.OUTPUT_SIZE)\n",
    "        else:\n",
    "            # otherwise chose epsilon-greedy action from neural net\n",
    "            action_index = self.get_predicted_action([state])\n",
    "        actual_action = self.action_map[action_index]\n",
    "        return action_index, actual_action\n",
    "\n",
    "    def get_predicted_action(self, sequence):\n",
    "        prediction = self.model.predict(np.array(sequence))[0]\n",
    "        # print(\"Prediction\", prediction)\n",
    "        return np.argmax(prediction)\n",
    "\n",
    "    def construct_memories(self):\n",
    "        # Get a minibatch of random samples from memory replay table\n",
    "        replay = random.sample(self.replay_memory, self.MINIBATCH_SIZE)\n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        states = np.array([step[0] for step in replay])\n",
    "        Q = self.model.predict(states)\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        new_states = np.array([step[3] for step in replay])\n",
    "        Q_next = self.model.predict(new_states)\n",
    "\n",
    "        X = []\n",
    "        Y = []\n",
    "\n",
    "        for index, (state, action, reward, state_, done) in enumerate(replay):\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_Q = np.amax(Q_next[index])\n",
    "                new_Q = reward + self.GAMMA * max_Q\n",
    "            else:\n",
    "                new_Q = reward\n",
    "\n",
    "            # Update the Q value for given state\n",
    "            target = Q[index]\n",
    "            target[action] = new_Q\n",
    "\n",
    "            # Append new values to training data\n",
    "            X.append(state)\n",
    "            Y.append(target)\n",
    "        return np.array(X), np.array(Y)\n",
    "\n",
    "    def train(self, is_terminal, step):\n",
    "        if not os.path.isdir('dqn'):\n",
    "            os.makedirs('dqn')\n",
    "\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if len(self.replay_memory) < self.MIN_MEMORY_SIZE:\n",
    "            return\n",
    "\n",
    "        # constructs training data for training of the neural network\n",
    "        X, y = self.construct_memories()\n",
    "\n",
    "        # Update target network counter after every episode\n",
    "        if is_terminal:\n",
    "            self.model.fit(\n",
    "                X,\n",
    "                y,\n",
    "                batch_size=self.MINIBATCH_SIZE,\n",
    "                verbose=1,\n",
    "                shuffle=False,\n",
    "                callbacks=[self.tensorboard]\n",
    "            )\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches a set value, update the target network with weights of main network\n",
    "        if self.target_update_counter > self.UPDATE_TARGET_LIMIT:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "\n",
    "def init():\n",
    "    # HYPER PARAMETERS TO SEARCH\n",
    "    hidden_layer_nodes = np.arange(32, 481, 32)  # 32, 64, 96, 128 ....\n",
    "    num_episodes = [2500, 5000, 10_000, 20_000]\n",
    "    replay_memory_size = [1000, 2000, 3000, 5000]\n",
    "    final_layer_activation = [\"linear\"]\n",
    "    minibatch_sizes = [16, 32, 64]\n",
    "    learning_rates = [1e-2, 1e-3, 1e-4]\n",
    "\n",
    "    for num_episode in num_episodes:\n",
    "        for nodes in hidden_layer_nodes:\n",
    "            for mem_size in replay_memory_size:\n",
    "                for minibatch in minibatch_sizes:\n",
    "                    for final_act in final_layer_activation:\n",
    "                        for lr in learning_rates:\n",
    "                            learn(nodes, num_episode, mem_size, final_act, minibatch, lr)\n",
    "\n",
    "\n",
    "# Train Q network for a DQN agent\n",
    "def learn(nodes=49, num_episodes=5000, memory_size=10_000, final_act=\"linear\", minibatch=32, lr=1e-2):\n",
    "    print(f\"LEARNING PARAMS: nodes={nodes} num_episodes={num_episodes} memory_size={memory_size} \" \\\n",
    "          f\"final_act={final_act} batch={minibatch} lr={lr}\")\n",
    "    game = Pixelcopter(width=250, height=250)\n",
    "    env = PLE(game, fps=30, display_screen=True, force_fps=True)\n",
    "    env.init()\n",
    "    episode_rewards = []\n",
    "    agent = DQNAgent(\"train\", nodes, memory_size, final_act, minibatch, lr, num_episodes)\n",
    "    interval = 100\n",
    "    offset = 1\n",
    "    # Save model, but only when min reward is greater or equal a set value\n",
    "    if not os.path.isdir(os.path.join(os.getcwd(), f\"dqn/{agent.MODEL_NAME}\")):\n",
    "        os.mkdir(os.path.join(os.getcwd(), f\"dqn/{agent.MODEL_NAME}\"))\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        print(\"Episode : \", episode)\n",
    "        agent.tensorboard.step = episode\n",
    "        done = False\n",
    "        step = 1\n",
    "        total_reward = 0.0\n",
    "        # initial state\n",
    "        state = np.array(list(env.getGameState().values()))\n",
    "        # print(\"State:\", state)\n",
    "        while not done:\n",
    "            if env.game_over():\n",
    "                print(\"GAME OVER!\")\n",
    "                print(\"Total reward\", total_reward)\n",
    "                done = True\n",
    "            action_index, action = agent.select_action(state)\n",
    "            action_string = 'jump!' if action_index == 1 else 'chill'\n",
    "            # print(\"Action:\", action, action_string)\n",
    "            reward = env.act(action)\n",
    "            # print(\"Reward:\", reward)\n",
    "            new_state = np.array(list(env.getGameState().values()))\n",
    "            # update total reward\n",
    "            total_reward += reward\n",
    "            # update replay memory\n",
    "            agent.update_replay_memory(state, action_index, reward, new_state, done)\n",
    "            # update q_network\n",
    "            agent.train(done, step)\n",
    "            # update current state with new state\n",
    "            state = new_state\n",
    "            # increment time step\n",
    "            step += 1\n",
    "        # perform update only if a new max. reward was obtained after an episode (allow offset value of 5 for current max reward)\n",
    "        if len(episode_rewards) > 0 and total_reward > (np.max(episode_rewards) - offset):\n",
    "            can_update = True\n",
    "        else:\n",
    "            can_update = False\n",
    "        # Append episode rewards to list of all episode rewards\n",
    "        episode_rewards.append(total_reward)\n",
    "        if can_update or episode == 1:\n",
    "            average_reward = np.mean(episode_rewards[-interval:])\n",
    "            min_reward = np.min(episode_rewards[-interval:])\n",
    "            max_reward = np.max(episode_rewards[-interval:])\n",
    "            agent.tensorboard.update_stats(\n",
    "                reward_avg=average_reward,\n",
    "                reward_min=min_reward,\n",
    "                reward_max=max_reward,\n",
    "                epsilon=agent.EPSILON\n",
    "            )\n",
    "            # Save model, but only when min reward is greater or equal a set value\n",
    "            agent.model.save(\n",
    "                f'dqn/{agent.MODEL_NAME}/{agent.MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f} avg_{min_reward:_>7.2f}min.h5')\n",
    "        # Decay epsilon\n",
    "        if agent.EPSILON > agent.MIN_EPSILON:\n",
    "            agent.EPSILON -= agent.DECAY_RATE\n",
    "            print(\"EPSILON\", agent.EPSILON)\n",
    "            # ensure epsilon does not subside below minimum value\n",
    "            agent.EPSILON = max(agent.MIN_EPSILON, agent.EPSILON)\n",
    "        env.reset_game()\n",
    "    plot_graph(episode_rewards, num_episodes, nodes, memory_size, final_act, minibatch, lr)\n",
    "\n",
    "\n",
    "def plot_graph(episode_rewards, num_episodes, nodes, memory_size, final_act, minibatch, lr):\n",
    "    fig, ax = plt.subplots(nrows=1, figsize=(12, 15))\n",
    "    episodes = np.arange(1, len(episode_rewards) + 1)\n",
    "    ax.plot(episodes, episode_rewards)\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.set_title(\"Learning curve for DQN agent\")\n",
    "    ax.set_xlabel(\"Number of episodes\")\n",
    "    ax.set_ylabel(\"Reward\")\n",
    "    plt.show()\n",
    "    plt.savefig(\n",
    "        f\"graphs/DQN learning curve - num_episodes={num_episodes} hidden_nodes={nodes} mem_size={memory_size} final_act={final_act} minibatch={minibatch} lr={lr}.png\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# run the game using best DQN model\n",
    "def play():\n",
    "    game = Pixelcopter(width=250, height=250)\n",
    "    env = PLE(game, fps=30, display_screen=True, force_fps=True)\n",
    "    env.init()\n",
    "    agent = DQNAgent(\"play\")\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    state = np.array(list(env.getGameState().values()))\n",
    "    while True:\n",
    "        if env.game_over():\n",
    "            print(\"===========================\")\n",
    "            print(\"TOTAL REWARD: \", total_reward)\n",
    "            step = 0\n",
    "            total_reward = 0\n",
    "            env.reset_game()\n",
    "\n",
    "        action_index, action = agent.select_action(state)\n",
    "        # obtime.sleep(0.05)\n",
    "        # action_string = 'jump!' if action_index == 1 else 'chill'\n",
    "        reward = env.act(action)\n",
    "        new_state = np.array(list(env.getGameState().values()))\n",
    "\n",
    "        # PRINT CURRENT STATS\n",
    "        # print(\"Current State:\", state)\n",
    "        # print(\"Action:\", action, action_string)\n",
    "        # print(\"Reward:\", reward)\n",
    "        # print(\"New State:\", new_state)\n",
    "\n",
    "        state = new_state\n",
    "        step += 1\n",
    "        total_reward += reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    play()\n",
    "    # learn(nodes=49, num_episodes=5000, lr=1e-2)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
